{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T22:10:37.946552Z",
     "start_time": "2024-11-14T22:10:37.838766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from car_racing import CarRacing\n",
    "!python --version"
   ],
   "id": "b9c2b69cdf29ac07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T22:13:29.806256Z",
     "start_time": "2024-11-14T22:13:26.367150Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117",
   "id": "8f28ba19399ead69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T19:24:56.769729Z",
     "start_time": "2024-11-18T19:24:56.491216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import gnwrapper\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "gymlogger.set_level(30)\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import base64\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "from merger import MultiCarRacing\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "343c306719deb09a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T19:24:57.215797Z",
     "start_time": "2024-11-18T19:24:57.207707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gym.register(\n",
    "    id='MultiCarRacing-v0',\n",
    "    entry_point='merger:MultiCarRacing',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=900\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id='CarRacingDiscrete-v0',\n",
    "    entry_point='merger:CarRacingDiscrete',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=900\n",
    ")\n"
   ],
   "id": "b6c671275f1309a6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T19:24:57.463359Z",
     "start_time": "2024-11-18T19:24:57.445867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make('MultiCarRacing-v0', num_agents=1)\n",
    "env_2 = gym.make('CarRacingDiscrete-v0')\n",
    "print(\"Observation Space Size: \", env_2.observation_space)\n",
    "print(\"Action Space Size: \", env_2.action_space)\n",
    "print(\"Observation Space Size: \", env.observation_space)\n",
    "print(\"Action Space Size: \", env.action_space)\n",
    "env.close()"
   ],
   "id": "8cce796d6ab3c029",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Size:  Box(96, 96, 3)\n",
      "Action Space Size:  Discrete(5)\n",
      "Observation Space Size:  Box(96, 96, 3)\n",
      "Action Space Size:  Discrete(5)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:08:43.360627Z",
     "start_time": "2024-11-18T02:08:43.108561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "\n",
    "class ProgressCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that displays training progress, tracks the best mean reward,\n",
    "    and automatically saves the model every 10 log intervals.\n",
    "\n",
    "    Args:\n",
    "        total_timesteps (int): The total number of timesteps in the training run.\n",
    "        log_interval (int): The interval (in timesteps) at which to log progress.\n",
    "        save_path (str): The path to save the model (this file will be overwritten each time).\n",
    "        verbose (int): Verbosity level (0: no output, 1: default output).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, total_timesteps, log_interval=1000, save_path=\"model\", verbose=1):\n",
    "        super(ProgressCallback, self).__init__(verbose)\n",
    "        self.start_time = time.time()\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.current_timesteps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.log_interval = log_interval\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.current_timesteps += 1\n",
    "\n",
    "        try:  # Try to access the reward; if it fails, it's likely the last step of training.\n",
    "            reward = self.locals[\"rewards\"][0]\n",
    "            self.episode_rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            if self.verbose > 0:\n",
    "                print(e)\n",
    "            pass  # Ignore the exception if reward isn't available (likely end of training).\n",
    "\n",
    "        if self.n_calls % self.log_interval == 0:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            remaining_timesteps = self.total_timesteps - self.current_timesteps\n",
    "\n",
    "            if self.current_timesteps > 0:\n",
    "                remaining_time = elapsed_time * (remaining_timesteps / self.current_timesteps)\n",
    "            else:\n",
    "                remaining_time = 0\n",
    "\n",
    "            remaining_time_hours = int(remaining_time // 3600)\n",
    "            remaining_time_minutes = int((remaining_time % 3600) // 60)\n",
    "            remaining_time_seconds = int(remaining_time % 60)\n",
    "\n",
    "            elapsed_time_hours = int(elapsed_time // 3600)\n",
    "            elapsed_time_minutes = int((elapsed_time % 3600) // 60)\n",
    "            elapsed_time_seconds = int(elapsed_time % 60)\n",
    "\n",
    "            mean_reward = np.mean(self.episode_rewards) if self.episode_rewards else None\n",
    "            self.episode_rewards = []\n",
    "\n",
    "            print(f\"Timesteps: {self.current_timesteps}/{self.total_timesteps}, \"\n",
    "                  f\"Remaining Timesteps: {remaining_timesteps}, \"\n",
    "                  f\"Elapsed Time: {elapsed_time_hours:02}:{elapsed_time_minutes:02}:{elapsed_time_seconds:02}, \"\n",
    "                  f\"Estimated Time Remaining: {remaining_time_hours:02}:{remaining_time_minutes:02}:{remaining_time_seconds:02}\")\n",
    "\n",
    "            if mean_reward is not None:\n",
    "                print(f\"Mean Reward (last {self.log_interval} steps): {mean_reward:.5f}\")\n",
    "\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    print(f\"New best mean reward: {self.best_mean_reward:.5f}\")\n",
    "\n",
    "            # Save the model every 10 log intervals (log_interval * 10)\n",
    "            if self.n_calls % (self.log_interval * 10) == 0:\n",
    "                if self.model is not None:\n",
    "                    self.model.save(self.save_path)\n",
    "                    print(f\"Model saved to {self.save_path} (overwritten)\")\n",
    "\n",
    "        return True\n"
   ],
   "id": "bbbe22b54ad3dfe8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:08:43.748521Z",
     "start_time": "2024-11-18T02:08:43.738293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonitorCustom(gym.wrappers.Monitor):\n",
    "    def __init__(self, env):\n",
    "        super(MonitorCustom, self).__init__(env, './video', force=True)"
   ],
   "id": "a4b9efa033570584",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:02:15.072772Z",
     "start_time": "2024-11-18T02:02:15.052581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.dqn import CnnPolicy\n",
    "from merger import CarRacingDiscrete, MultiCarRacing\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "VERSION = \"1M\"\n",
    "NUM_OF_STEPS = 1_000_000\n",
    "LOG_INTERVAL = 1000\n",
    "# BUFFER_SIZE = 150000\n",
    "LEARNING_STARTS = 5000\n",
    "MODEL_SAVE_NAME = \"PPO_Discrete_RL_\" + str(VERSION)\n",
    "SAVED_MODEL_VERSION = \"latest\"\n",
    "LOAD_SAVED_MODEL = False\n",
    "\n",
    "env = gym.make(\"MultiCarRacing-v0\",\n",
    "               num_agents=1, verbose=0, use_ego_color=True,\n",
    "               )\n",
    "\n",
    "env = MonitorCustom(env)\n",
    "\n",
    "# if LOAD_SAVED_MODEL:\n",
    "#     try:\n",
    "# \n",
    "#         PPOmodel = PPO.load(MODEL_SAVE_NAME, env=env)\n",
    "#         print(\"LOAD SAVED PPÎŸ MODEL\")\n",
    "# \n",
    "#     except:\n",
    "#         print(\"NO MODEL FOUND\")\n",
    "# else:\n",
    "#     if 'PPOmodel' not in globals():\n",
    "#         PPOmodel = PPO('CnnPolicy', env, verbose=1, ent_coef=0.005)\n",
    "#         print(\"INITIALIZE NEW PPO Discrete MODEL\")\n",
    "#     else:\n",
    "PPOmodel = PPO.load(\"PPO_Discrete_RL_2M.zip\", env=env)\n",
    "\n",
    "print(\"CONTINUE PPO Discrete MODEL TRAINING\")\n",
    "\n",
    "progress_callback = ProgressCallback(total_timesteps=NUM_OF_STEPS, verbose=1, save_path=\"TRAIN_PPO_Discrete_RL_\" + str(VERSION))\n",
    "\n",
    "PPOmodel.learn(total_timesteps=NUM_OF_STEPS, log_interval=LOG_INTERVAL, callback=progress_callback)\n",
    "PPOmodel.save(MODEL_SAVE_NAME)"
   ],
   "id": "dbecf2c4943733bb",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MonitorCustom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_26132\\762906036.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m                )\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[0menv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMonitorCustom\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;31m# if LOAD_SAVED_MODEL:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'MonitorCustom' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:08:46.875882Z",
     "start_time": "2024-11-18T02:08:46.867341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_version(model, env, version_name, video_path):\n",
    "    MODEL_SAVE_NAME = version_name\n",
    "\n",
    "    try:\n",
    "        loaded_model = model.load(MODEL_SAVE_NAME)\n",
    "        print(f\"Model {MODEL_SAVE_NAME} loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    env = MonitorCustom(env)\n",
    "    obs = env.reset()\n",
    "    print(\"Observation shape: \", obs.shape)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        obs = np.copy(obs)\n",
    "\n",
    "        action, _states = loaded_model.predict(obs, deterministic=True)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "    env.close()"
   ],
   "id": "4a946fe337050581",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:11:13.735859Z",
     "start_time": "2024-11-18T02:10:53.032607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import DQN, PPO\n",
    "from merger import CarRacingDiscrete, MultiCarRacing\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.atari_wrappers import WarpFrame\n",
    "\n",
    "# env = CarRacingDiscrete()\n",
    "# evaluate_version(DQN, env, \"DQN_RL_54\", \"v28\", \"./video\")\n",
    "\n",
    "# env_str = \"MultiCarRacing-v0\"\n",
    "# env_kwargs_dict = {\"num_agents\": 1, \"verbose\": 0}\n",
    "\n",
    "# env_eval = make_vec_env(env_str, n_envs=1, env_kwargs=env_kwargs_dict, wrapper_class=WarpFrame)\n",
    "# env_eval = VecFrameStack(env_eval, n_stack=4)\n",
    "# env_eval = VecTransposeImage(env_eval)\n",
    "\n",
    "env = gym.make('MultiCarRacing-v0', num_agents=1, \n",
    "               use_ego_color=True)\n",
    "evaluate_version(PPO, env, \"PPO_Discrete_RL_2M\", \"./video\")"
   ],
   "id": "5b7a35478f4c0438",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model PPO_Discrete_RL_2M loaded successfully\n",
      "Track generation: 1137..1422 -> 285-tiles track\n",
      "Observation shape:  (96, 96, 3)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-18T19:51:10.311081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "model = PPO.load(\"PPO_Discrete_RL_1M\")\n",
    "\n",
    "env = gym.make('MultiCarRacing-v0', num_agents=1)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        obs = np.copy(obs)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "\n",
    "    episode_rewards.append(total_rewards)\n",
    "\n",
    "end_time = time.time()\n",
    "evaluation_time = end_time - start_time\n",
    "print(f\"Total evaluation time: {evaluation_time:.2f} seconds\")\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "\n",
    "window_size = 10\n",
    "rewards_smoothed = pd.Series(episode_rewards).rolling(window=window_size).mean()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rewards_smoothed, label=f'Smoothed Rewards (window={window_size})')\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.title(f\"Performance on {env.spec.id}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5ddd79323ee7b3a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Track generation: 1128..1414 -> 286-tiles track\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab33b7bc1125ec23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
